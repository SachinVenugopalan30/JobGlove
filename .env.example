OPENAI_API_KEY=your_openai_key_here
GEMINI_API_KEY=your_gemini_key_here
ANTHROPIC_API_KEY=your_claude_key_here
GROQ_API_KEY=your_groq_key_here

# Ollama Configuration (for local models)
OLLAMA_ENABLED=false
OLLAMA_BASE_URL=http://localhost:11434

# AI Model Configuration
# OpenAI Models: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
OPENAI_MODEL=gpt-4o-mini

# Gemini Models: gemini-2.5-flash, gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash
GEMINI_MODEL=gemini-2.5-flash

# Claude Models: claude-haiku-4-5-20251001, claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022, claude-3-opus-20240229
CLAUDE_MODEL=claude-haiku-4-5-20251001

# Groq Models: llama-3.3-70b-versatile, llama-3.3-70b-specdec, llama-3.1-70b-versatile, mixtral-8x7b-32768
GROQ_MODEL=llama-3.3-70b-versatile

# Ollama Models: llama3.2, llama3.1, mistral, phi3 (must be pulled first: ollama pull <model>)
OLLAMA_MODEL=llama3.2

MAX_FILE_SIZE=10485760
DEFAULT_USER_NAME=User

# Debug - set to true to save raw AI responses to backend/debug_responses/
DEBUG_AI_RESPONSES=false
